# DSAA6000Q Assignment 3: Self-Instruct Pipeline

This repository contains the implementation of a self-instruct pipeline for language model fine-tuning, following the approach described in the Self-Instruct paper. The pipeline consists of four main steps:

1. Inverse SFT: Training a backward model to generate instructions from completions
2. Self-Augmentation: Using the backward model to generate new instructions
3. Self-Curation: Filtering high-quality instruction-response pairs
4. Model Fine-tuning: Training the final model on curated data

## Model Selection

This implementation uses the following models:
- Base model: Llama 3.1 8B, a powerful and efficient open-source LLM
- Evaluation model for curation: Meta's Llama 3.1-8B-chat for quality assessment
- The fine-tuned models maintain the same architecture as the base model while optimizing for specific tasks

## Unsloth Framework

All fine-tuning in this project is powered by [Unsloth](https://github.com/unslothai/unsloth), a highly optimized library specifically designed for efficient LLM fine-tuning:

- **Speed Optimization**: Unsloth accelerates training by 2-5x compared to standard fine-tuning approaches
- **Memory Efficiency**: Uses 8-bit and 4-bit quantization to reduce VRAM requirements
- **LoRA Integration**: Uses Low-Rank Adaptation method for parameter-efficient fine-tuning
- **QLoRA Support**: Implements Quantized LoRA for even more efficient training


These values can be adjusted in the respective training scripts to optimize for different training objectives.

## Scripts

The scripts should be run in the following order:

### 1. `scripts/inverse_sft.py`

This script finetunes Llama 3.1 8B as a backward model that predicts instructions from completions. It uses the OpenAssistant-Guanaco dataset to train a model that can generate instructions given responses.

```bash
python scripts/inverse_sft.py
```

**Hugging Face Model**: [ssui-liu/backwards-guanaco-llama3.1-8b-sft](https://huggingface.co/ssui-liu/backwards-guanaco-llama3.1-8b-sft)

### 2. `scripts/self_augmentation_lima.py`

This script randomly samples 150 completions from the LIMA dataset (filtering out multi-turn examples) and uses the backward model to generate instructions for these completions.

```bash
python scripts/self_augmentation_lima.py
```

The script produces `data/self_augmented.json` containing instruction-response pairs where:
- Instructions are generated by the backward model
- Responses are from the original LIMA dataset

### 3. `scripts/self_curation.py`

This script evaluates the quality of the generated instruction-response pairs using few-shot prompting with the Llama-7b-chat model. It rates each pair on a scale of 1-5 and filters to retain only high-quality examples.

```bash
python scripts/self_curation.py
```

**Hugging Face Dataset**: [ssui-liu/curated-augmented-lima](https://huggingface.co/datasets/ssui-liu/curated-augmented-lima)

### 4. `scripts/finetune_model.py`

This script finetunes the base Llama 3.1 8B model on the curated dataset of high-quality instruction-response pairs.

```bash
python scripts/finetune_model.py
```

**Hugging Face Model**: [ssui-liu/llama3.1-8b-lima-sft](https://huggingface.co/ssui-liu/llama3.1-8b-lima-sft)

## Data Files

### Preprocessing
- `data/preprocessing.ipynb`: Jupyter notebook containing data preprocessing steps and exploratory analysis of the datasets

### Datasets
- `data/lima_single_turn.json`: Single-turn examples extracted from the LIMA dataset
- `data/guanaco_inverse.json`: OpenAssistant-Guanaco dataset used for training the backward model
- `data/self_augmented.json`: Generated instruction-response pairs from the self-augmentation step
- `data/curated_dataset.json`: Initial curated dataset with quality ratings
- `data/curated_dataset_all_evaluated.json`: Complete evaluated dataset with all quality metrics 

## Requirements

The project dependencies are specified in `requirements.txt`. Key dependencies include:

```
unsloth>=2024.5           # Efficient fine-tuning library
torch>=2.2.0              # PyTorch for deep learning
transformers>=4.35.0      # Hugging Face Transformers library
datasets>=2.14.0          # Hugging Face Datasets library
peft>=0.6.0               # Parameter-Efficient Fine-Tuning
accelerate>=0.23.0        # Distributed training utilities
bitsandbytes>=0.41.0      # Quantization utilities
wandb>=0.15.0             # Weights & Biases for experiment tracking
trl>=0.7.2                # Transformer Reinforcement Learning
```

To install all dependencies:

```bash
pip install -r requirements.txt
```

Experiments were run on an HPC server featuring a single **NVIDIA A800 GPU** with 80GB of memory.