# DSAA6000Q Assignment 3: Self-Instruct Pipeline

This repository contains the implementation of a self-instruct pipeline for language model fine-tuning, following the approach described in the Self-Instruct paper. The pipeline consists of four main steps:

1. Inverse SFT: Training a backward model to generate instructions from completions
2. Self-Augmentation: Using the backward model to generate new instructions
3. Self-Curation: Filtering high-quality instruction-response pairs
4. Model Fine-tuning: Training the final model on curated data

## Scripts

The scripts should be run in the following order:

### 1. `scripts/inverse_sft.py`

This script finetunes Llama2 7B as a backward model that predicts instructions from completions. It uses the OpenAssistant-Guanaco dataset to train a model that can generate instructions given responses.

```bash
python scripts/inverse_sft.py
```

**Hugging Face Model**: [YOUR_USERNAME/llama2-7b-backward](https://huggingface.co/YOUR_USERNAME/llama2-7b-backward)

### 2. `scripts/self_augmentation_lima.py`

This script randomly samples 150 completions from the LIMA dataset (filtering out multi-turn examples) and uses the backward model to generate instructions for these completions.

```bash
python scripts/self_augmentation_lima.py
```

The script produces `data/self_augmented.json` containing instruction-response pairs where:
- Instructions are generated by the backward model
- Responses are from the original LIMA dataset

### 3. `scripts/self_curation.py`

This script evaluates the quality of the generated instruction-response pairs using few-shot prompting with the Llama-7b-chat model. It rates each pair on a scale of 1-5 and filters to retain only high-quality examples.

```bash
python scripts/self_curation.py
```

**Hugging Face Dataset**: [YOUR_USERNAME/self-instruct-curated](https://huggingface.co/datasets/YOUR_USERNAME/self-instruct-curated)

### 4. `scripts/finetune_model.py`

This script finetunes the base Llama2 7B model on the curated dataset of high-quality instruction-response pairs.

```bash
python scripts/finetune_model.py
```

**Hugging Face Model**: [YOUR_USERNAME/llama2-7b-self-instruct](https://huggingface.co/YOUR_USERNAME/llama2-7b-self-instruct)

## Data Files

### Preprocessing
- `data/preprocessing.ipynb`: Jupyter notebook containing data preprocessing steps and exploratory analysis of the datasets

### Datasets
- `data/lima_single_turn.json`: Single-turn examples extracted from the LIMA dataset
- `data/guanaco_inverse.json`: OpenAssistant-Guanaco dataset used for training the backward model
- `data/self_augmented.json`: Generated instruction-response pairs from the self-augmentation step
- `data/curated_dataset.json`: Initial curated dataset with quality ratings
- `data/curated_dataset_all_evaluated.json`: Complete evaluated dataset with all quality metrics 